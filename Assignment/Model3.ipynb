{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, \n",
    "    Dropout, BatchNormalization, Input, \n",
    "    LeakyReLU, RandomFlip, RandomRotation, \n",
    "    RandomZoom, Rescaling\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    EarlyStopping, \n",
    "    ReduceLROnPlateau, \n",
    "    CSVLogger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_HEIGHT = 224  # Increased from original 188\n",
    "IMG_WIDTH = 224   # Increased from original 188\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50       # Increased from original\n",
    "TEST_SIZE = 0.2\n",
    "LEARNING_RATE = 1e-4  # Adjusted learning rate\n",
    "PATIENCE = 15     # Increased patience\n",
    "L2_RATE = 1e-4    # L2 regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directories\n",
    "BASE_DIR = '../'\n",
    "INPUT_DIR = os.path.join(BASE_DIR, 'input')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "SUBDIR = 'flower_photos'\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess dataset\n",
    "def load_dataset(data_dir):\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        subset=\"training\",\n",
    "        seed=RANDOM_STATE,\n",
    "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=TEST_SIZE,\n",
    "    )\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        subset=\"validation\",\n",
    "        seed=RANDOM_STATE,\n",
    "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=TEST_SIZE,\n",
    "    )\n",
    "    \n",
    "    return train_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess dataset\n",
    "def load_dataset(data_dir):\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        subset=\"training\",\n",
    "        seed=RANDOM_STATE,\n",
    "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=TEST_SIZE,\n",
    "    )\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        subset=\"validation\",\n",
    "        seed=RANDOM_STATE,\n",
    "        image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=TEST_SIZE,\n",
    "    )\n",
    "    \n",
    "    return train_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Performance optimization for dataset\n",
    "def configure_dataset(ds, augment=False):\n",
    "    if augment:\n",
    "        # Data augmentation layers\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            RandomFlip('horizontal'),\n",
    "            RandomRotation(0.2),\n",
    "            RandomZoom(0.2),\n",
    "        ])\n",
    "        \n",
    "        ds = ds.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y), \n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "    return ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes, l2_rate=1e-4):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Preprocessing\n",
    "    x = Rescaling(1./255)(inputs)\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    x = Conv2D(64, (3,3), padding='same', kernel_regularizer=l2(l2_rate))(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    x = Conv2D(128, (3,3), padding='valid', kernel_regularizer=l2(l2_rate))(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), padding='valid')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    x = Conv2D(256, (3,3), padding='valid', kernel_regularizer=l2(l2_rate))(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), padding='valid')(x)\n",
    "    x = Dropout(0.35)(x)\n",
    "    \n",
    "    # Fourth Convolutional Block\n",
    "    x = Conv2D(512, (3,3), padding='valid', kernel_regularizer=l2(l2_rate))(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), padding='valid')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Fully Connected Layers \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Head Layers\n",
    "    x = Dense(512, kernel_regularizer=l2(l2_rate))(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_regularizer=l2(l2_rate))(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main training function\n",
    "def train_model():\n",
    "    # Load dataset\n",
    "    data_dir = os.path.join(INPUT_DIR, SUBDIR)\n",
    "    train_ds, test_ds = load_dataset(data_dir)\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = train_ds.class_names\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    # Configure datasets\n",
    "    train_ds = configure_dataset(train_ds, augment=True)\n",
    "    test_ds = configure_dataset(test_ds)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_cnn_model(\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), \n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    # Model Summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    # checkpoint_path = os.path.join(MODEL_DIR, SUBDIR, 'best_custom_model.keras')\n",
    "    checkpoint_path = os.path.normpath(os.path.join(MODEL_DIR, SUBDIR, 'best_custom_model.keras'))\n",
    "\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=8,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    csv_logger = CSVLogger(\n",
    "        os.path.join(OUTPUT_DIR, 'custom_cnn_training_log.csv'), \n",
    "        separator=',', \n",
    "        append=False\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=test_ds,\n",
    "        callbacks=[\n",
    "            model_checkpoint, \n",
    "            early_stopping, \n",
    "            reduce_lr,\n",
    "            csv_logger\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "    print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'custom_cnn_training_history.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model():\n",
    "    # Load dataset\n",
    "    data_dir = os.path.join(INPUT_DIR, SUBDIR)\n",
    "    train_ds, test_ds = load_dataset(data_dir)\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = train_ds.class_names\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    # Configure datasets\n",
    "    train_ds = configure_dataset(train_ds, augment=True)\n",
    "    test_ds = configure_dataset(test_ds)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_cnn_model(\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), \n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    # Compile model with careful learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3),  # Start with a slightly lower learning rate\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks with more patience\n",
    "    checkpoint_path = os.path.normpath(os.path.join(MODEL_DIR, SUBDIR, 'best_improved_model.keras'))\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,  # Increased patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,  # Increased patience for learning rate reduction\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    csv_logger = CSVLogger(\n",
    "        os.path.join(OUTPUT_DIR, 'improved_cnn_training_log.csv'), \n",
    "        separator=',', \n",
    "        append=False\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=50,  # Increased max epochs \n",
    "        validation_data=test_ds,\n",
    "        callbacks=[\n",
    "            model_checkpoint, \n",
    "            early_stopping, \n",
    "            reduce_lr,\n",
    "            csv_logger\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "    print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n",
    "    \n",
    "    # Visualization code remains the same as in previous implementation\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n",
      "Epoch 1/50\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3736 - loss: 2.1515\n",
      "Epoch 1: val_accuracy improved from -inf to 0.32698, saving model to ..\\models\\flower_photos\\best_improved_model.keras\n",
      "\u001b[1m367/367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 1s/step - accuracy: 0.3737 - loss: 2.1510 - val_accuracy: 0.3270 - val_loss: 1.8953 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m 60/367\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:26\u001b[0m 1s/step - accuracy: 0.4354 - loss: 1.8313"
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "if __name__ == '__main__':\n",
    "    model, history = train_model() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
