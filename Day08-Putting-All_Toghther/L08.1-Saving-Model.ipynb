{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    " \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from utils.helper import fn_plot_tf_hist, fn_plot_confusion_matrix\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from utils.helper import fn_plot_torch_hist, fn_plot_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "inpDir = '../input' # Input Stored here\n",
    "outDir = '../ouput' # output Here\n",
    "modelDir = '../models'# to save Models\n",
    "subDir = 'fifa_2019' # sub dir by dataset\n",
    "RANDOM_STATE = 24\n",
    "np.random.RandomState(seed = RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "#rng = np.random.default_rng(seed = RANDOM_STATE)\n",
    "#N_SAMPLE = 1000\n",
    "TEST_SIZE = 0.2\n",
    "ALPHA = 0.001 # learning rate\n",
    "NOISE = 0.2 # Error\n",
    "EPOCHS = 101\n",
    "BATCH_SIZE = 256\n",
    "LR_FACTOR=0.1\n",
    "LR_PATIENCE=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the plotting parameters\n",
    "params = {\n",
    "    'legend.fontsize': 'medium',\n",
    "    'figure.figsize':(15,4),\n",
    "    'axes.labelsize':'medium',\n",
    "    'axes.titlesize':'medium',\n",
    "    'xtick.labelsize': 'medium',\n",
    "    'ytick.labelsize':'medium',\n",
    "    #'text.usetex':True,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "CMAP = plt.cm.coolwarm\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.read_csv(os.path.join(inpDir,'fifa_2019.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = os.path.join(inpDir, 'fashion_mnist', 'fashion-mnist_train.csv')\n",
    "test_filename = os.path.join(inpDir, 'fashion_mnist', 'fashion-mnist_test.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_filename, header = 0)\n",
    "test_df = pd.read_csv(test_filename, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum().sum(), test_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "#               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "class_names = {0: 'T-shirt/top',1:'Trouser',2:'Pullover',3:'Dress',4:'Coat',\n",
    "               5:'Sandal', 6: 'Shirt',7: 'Sneaker', 8:'Bag', 9: 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample DATA plot: Display a grid of images from the dataset along with their labels\n",
    "\n",
    "n_rows =  8 # Number of rows to display in the grid\n",
    "n_cols = 8 # Number of columns\n",
    "\n",
    "# randomly sample 'n_rows' * n_cols images from the dataset\n",
    "plot_df = train_df.sample(n = n_rows * n_cols)\n",
    "\n",
    "#create a new figure for the grid of images with a specified size\n",
    "fig = plt.figure(figsize  =(15,12))\n",
    "\n",
    "#Adjust Margins\n",
    "fig.subplots_adjust(left = 0, right = 1, bottom=0, top = 1, hspace =0.05, wspace = 0.05)\n",
    "i= 0 \n",
    "for idx, row in plot_df.iterrows():\n",
    "    i += 1\n",
    "    image = row.values[1:].reshape(28,28)\n",
    "\n",
    "    ax = fig.add_subplot(n_rows, n_cols, i, xticks=[], yticks=[])\n",
    "    #Display the image on the subplot using a binary colormap\n",
    "    ax.imshow(image, cmap = plt.cm.binary, interpolation = 'nearest')\n",
    "\n",
    "    ax.text(2,4, str(row.iloc[0]), color = 'b', fontsize=16)\n",
    "    #\n",
    "    ax.text(2,25, class_names[row.iloc[0]], color='r', fontsize  = 16)\n",
    "#Display the entire grid of images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data_df.drop('Position',axis=1)\n",
    "# y = data_df['Position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.dropna(subset= ('Position'), inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_df.columns:\n",
    "    unq = train_df[col].unique()\n",
    "    print(f'{col}, #:{len(unq)}, Values:{unq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cols = data_df.select_dtypes(exclude='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cols.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = train_df.drop('label',axis=1).to_numpy()\n",
    "y_data = train_df['label'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data in test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=TEST_SIZE,stratify = y_data, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = test_df.drop('label', axis=1).to_numpy()\n",
    "y_valid = test_df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train), type(X_test), type(y_train), type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, X_valid.shape, y_train.shape, y_test.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data beetween [0-1]\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid = X_valid / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to delete the data which not useful any more\n",
    "# del train_df, test_df, X_train , X_test, X_valid, X_data, y_data\n",
    "# gc.collect()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm = MinMaxScaler()\n",
    "# X_train = mm.fit_transform(X_train)\n",
    "# X_test = mm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define custom dataset'''\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(MNISTDataset, self).__init__()\n",
    "        self.X = torch.tensor(X, dtype = torch.float32)\n",
    "        self.y = torch.tensor(y, dtype = torch.long)\n",
    "    def __len__(self): # length of the data = no. of rows \n",
    "        return(len(self.X))\n",
    "\n",
    "    def __getitem__(self, idx): # give me index of X , y\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch , y_batch = next(iter(train_loader))\n",
    "X_batch.shape, y_batch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNISTDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")\n",
    "X_batch, y_batch = next(iter(test_loader))\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "class Model(nn.Module):\n",
    "    # Dropout\n",
    "    #batchNorm\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        dor = [0.2,0.3,0.4,0.5,0.6]\n",
    "        #set1   \n",
    "        self.layer1 = nn.Linear(input_dim, 392)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.do1 = nn.Dropout(dor[0])\n",
    "        self.bn1 = nn.BatchNorm1d(392) # layer 1 output \n",
    "        \n",
    "        #set2\n",
    "        self.layer2 = nn.Linear(392,196)\n",
    "        self.actv2 = nn.ReLU()\n",
    "        self.do2 = nn.Dropout(dor[1])\n",
    "        self.bn2 = nn.BatchNorm1d(196)\n",
    "\n",
    "        #set 3\n",
    "\n",
    "        self.layer3 = nn.Linear(196,98)\n",
    "        self.actv3 = nn.ReLU()\n",
    "        self.do3 = nn.Dropout(dor[2])\n",
    "        self.bn3 = nn.BatchNorm1d(98)\n",
    "\n",
    "        #set4\n",
    "        self.layer4 = nn.Linear(98,49)\n",
    "        self.actv4 = nn.ReLU()\n",
    "        self.do4 = nn.Dropout(dor[0])\n",
    "        self.bn4 = nn.BatchNorm1d(49)\n",
    "\n",
    "        #set5\n",
    "        self.layer5 = nn.Linear(49,24)\n",
    "        self.actv5 = nn.ReLU()\n",
    "        self.do5 = nn.Dropout(dor[3])\n",
    "        self.bn5 = nn.BatchNorm1d(24)\n",
    "        \n",
    "        #ouput\n",
    "        self.layer6 = nn.Linear(24,10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #Set1\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.actv1(x)\n",
    "        x = self.do1(x)\n",
    "\n",
    "        #Set2\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = self.actv2(x)\n",
    "        x = self.do2(x)\n",
    "        #Set3\n",
    "        x = self.layer3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.actv3(x)\n",
    "        x = self.do3(x)\n",
    "        #Set4\n",
    "        x = self.layer4(x)\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        x = self.actv4(x)\n",
    "        x = self.do4(x)\n",
    "        #Set5\n",
    "        x = self.layer5(x)\n",
    "        x = self.bn5(x)\n",
    "        \n",
    "        x = self.actv5(x)\n",
    "        x = self.do5(x)\n",
    "       \n",
    "        #Set6\n",
    "        x = self.layer6(x)\n",
    "        return x\n",
    "model = Model(input_dim).to(device=device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in list(model.parameters()):\n",
    "    print(param.shape)\n",
    "\n",
    "# Input\n",
    "#Bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = os.path.join(modelDir, subDir, f'torch_fifa_{torch.version.cuda}.pth')\n",
    "modelPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "#Compilation and # Fit()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = ALPHA)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, # learning rate decay\n",
    "    mode = 'min',\n",
    "    factor = LR_FACTOR,\n",
    "    patience = LR_PATIENCE,\n",
    "    min_lr = 1e-5\n",
    ")\n",
    "\n",
    "\n",
    "# always need to write \n",
    "# some lists to collect progress\n",
    "loss = []\n",
    "tloss = []\n",
    "n_epoch = []\n",
    "acc = []\n",
    "tacc=[]\n",
    "best_loss = np.inf\n",
    "# loop for definedd epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() # set model in training mode \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    tepoch_loss = 0\n",
    "    tepoch_acc = 0\n",
    "   \n",
    "    for batch_idx, (train_X, train_y) in enumerate(train_loader):\n",
    "        train_X = train_X.to(device)\n",
    "        train_y = train_y.to(device)\n",
    "        predict_proba = model(train_X) # make predict\n",
    "        batch_loss = loss_fn(predict_proba, train_y) # calculate loss\n",
    "        epoch_loss += (batch_loss - epoch_loss) / (batch_idx+1) # calculate running mean -> batch loss  for each batch -> then calculate avg loss for epoch\n",
    "        #curr_loss = loss_fn(predict_proba, train_y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # Gradient set to Zero\n",
    "        batch_loss.backward() # calculate loss in backpropagation\n",
    "        optimizer.step() # move with the steps given by optimizer\n",
    "        y_pred = predict_proba.argmax(dim=1).cpu().numpy()\n",
    "        batch_acc = accuracy_score(train_y.cpu().numpy(), y_pred)\n",
    "        epoch_acc += (batch_acc - epoch_acc)/(batch_idx+1) #accuracy for each batch -> then calculate avg accuracy for epoch\n",
    "        \n",
    "    loss.append(epoch_loss.data.item())\n",
    "    acc.append(epoch_acc)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, (test_X, test_y) in enumerate(test_loader):\n",
    "        test_X, test_y = test_X.to(device), test_y.to(device)\n",
    "        test_proba = model(test_X)\n",
    "        batch_loss = loss_fn(test_proba, test_y)\n",
    "        tepoch_loss += (batch_loss - tepoch_loss)/ (batch_idx + 1)\n",
    "\n",
    "        y_pred =test_proba.argmax(dim=1).cpu().numpy()\n",
    "        batch_acc = accuracy_score(test_y.cpu().numpy(), y_pred)\n",
    "        tepoch_acc += (batch_acc - tepoch_acc)/(batch_idx+1) #accuracy for each batch -> then calculate avg accuracy for epoch\n",
    "    \n",
    "    tacc.append(tepoch_acc)\n",
    "    tloss.append(tepoch_loss.data.item())\n",
    "    n_epoch.append(epoch)\n",
    "\n",
    "    #LR Reduction Step\n",
    "    scheduler.step(tepoch_loss)\n",
    "\n",
    "    if tepoch_loss < best_loss:\n",
    "        best_loss = tepoch_loss\n",
    "        torch.save(model,modelPath)\n",
    "    \n",
    "    # loss.append(curr_loss.data.item())\n",
    "    # y_pred = torch.argmax(predict_proba, dim=1).cpu().numpy()\n",
    "    # curr_acc = accuracy_score(train_y.cpu().numpy(), y_pred)\n",
    "    # acc.append(curr_acc)\n",
    "\n",
    "    # model.eval()# set your model in eval mode\n",
    "    # test_proba = model(test_X) # make prediction\n",
    "    # test_loss = loss_fn(test_proba, test_y) # calculate loss\n",
    "    # tloss.append(test_loss.data.item()) # append for plotting\n",
    "    \n",
    "    # y_pred = torch.argmax(test_proba, dim=1).cpu().numpy()\n",
    "    # test_acc = accuracy_score(test_y.cpu().numpy(),y_pred)\n",
    "    # tacc.append(test_acc)\n",
    "    # n_epoch.append(epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch:{epoch:>5d} | Loss: {epoch_loss:0.5f}/{tepoch_loss:0.5f}')\n",
    "        print(f' Accuracy: {epoch_acc:0.5f}/{tepoch_acc:0.5f}')\n",
    "        print(f'LR:{scheduler.get_last_lr()[0]:.5f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame({ \n",
    "  'epoch':n_epoch ,\n",
    "  'loss':loss,\n",
    "  'test_loss':tloss,\n",
    "  'acc':acc,\n",
    "  'test_acc':tacc\n",
    "})\n",
    "\n",
    "loss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_plot_torch_hist(hist_df=loss_df) # drop 0.3 0.2 0.2 0.2 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.load(modelPath, weights_only = False)\n",
    "model1.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.to(device)\n",
    "train_y = train_y.to(device)\n",
    "predict_proba = model1(train_X) \n",
    "y_pred = torch.argmax(predict_proba, dim=1).cpu().numpy()\n",
    "curr_acc = accuracy_score(train_y.cpu().numpy(), y_pred)\n",
    "print(curr_acc)\n",
    "\n",
    "\n",
    "test_X = test_X.to(device)\n",
    "test_y = test_y.to(device)\n",
    "test_proba = model1(test_X) \n",
    "y_pred = torch.argmax(test_proba, dim=1).cpu().numpy()\n",
    "test_acc = accuracy_score(test_y.cpu().numpy(),y_pred)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
